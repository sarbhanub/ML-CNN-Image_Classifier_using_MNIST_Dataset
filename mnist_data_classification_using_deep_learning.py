# -*- coding: utf-8 -*-
"""MNIST Data - Classification using Deep Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1McyxHn9uqAdS3j19Seu50Y3XmBxmH0Vr

#A multi layered fully Connected Neural Network to classify images from MNIST Dataset

##Introduction

### Dataset Information
This MNIST dataset consists of 70,000 images of handwritten digits where each digit of size 28 x 28 is flattened into a vector of size 784. These 70,000 images are split into training set and testing set where the training set consists of 60,000 images and the test set consists of 10,000 images.

###Library Imports

**Imports:** TensprFlow, NumPy, Pandas, matplotlib, keras, scikit-learn, seaborn
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import keras
from keras.layers import Dense, Dropout
from sklearn.metrics import confusion_matrix
import seaborn as sns

np.random.seed(0) # for constant results

"""##Importing MNIST Dataset"""

from keras.datasets import mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data() # downloading data from tf datasets

"""###Checking Data"""

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

"""###Visualization"""

num_classes = 10
f, ax = plt.subplots(1, num_classes, figsize=(28,28))
for i in range(0, num_classes):
  sample = x_train[y_train == i][0]
  ax[i].imshow(sample, cmap='gray')
  ax[i].set_title(format(i), fontsize=14)

for i in range(10):
  print(y_train[i], end=" ")

"""##Processing the Data

###Changing the Representation

A binary matrix representation of the data for Classification.
"""

y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

"""###Vector Map
Printing the categorigal data.
"""

for i in range(10):
  print(y_train[i])

"""Normalising the Data"""

x_train = x_train/255.0
x_test = x_test/255.0

x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

print(x_train.shape)

"""##Building the Model

Summary of The Model at the Output
"""

cnnmodel = keras.models.Sequential()
cnnmodel.add(Dense(units=128, input_shape=(784,), activation='relu'))
cnnmodel.add(Dense(units=128, activation='relu'))
cnnmodel.add(Dropout(0.25))
cnnmodel.add(Dense(units=10, activation='softmax'))
cnnmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
cnnmodel.summary()

"""## Fitting the Model with our Data
here x_train is the input and y_train output layer of our model.
"""

size = 128
epochs = 20
hist = cnnmodel.fit(x=x_train, y=y_train, batch_size=size, epochs=epochs)

"""Checking for **Loss** and **Accuracy** of our trained model."""

loss, accuracy = cnnmodel.evaluate(x_test, y_test)
print("Loss: {}, Accuracy: {}".format(loss, accuracy))

"""### Plotting the Model Fitting
Plotting the data for a better understanding.

You can see how the loss is reducing after every epoch and subsequently how the accuracy is increasing w.r.t. epoch.



"""

metrics = pd.DataFrame(hist.history)
metrics[['loss', 'accuracy']].plot()

"""###Prediction Table

From this table we can see the prediction status for the x_test data. One Row value of the matrix is high. Determining the output for the o/p layer of our trained NN.
"""

y_predict = cnnmodel.predict(x_test)
y_predict_classes = np.argmax(y_predict, axis=1)
print(y_predict)
print(y_predict_classes)

"""## Chceking the Model
Randomly choosing a field to see if the prediction is correct or not. Our model has almost 98% accuracy and in this case this is predicting the correct value.
"""

random = np.random.choice(len(x_test))
x_sample = x_test[random]
y_true = np.argmax(y_test, axis=1)
y_sample_true = y_true[random]
y_sample_predicted_class = y_predict_classes[random]

plt.imshow(x_sample.reshape(28, 28), cmap='gray')
plt.title("Predicted: {}, Original: {}".format(y_sample_predicted_class, y_sample_true), fontsize=14)

"""## Confusion Matrix

Confusion Matrix to see how our Classifier is operating.
Here We can individually see how our model is performing against different digits.
"""

cf_mat = confusion_matrix(y_true, y_predict_classes)
fig, axis = plt.subplots(figsize=(15,10))
ax = sns.heatmap(cf_mat, annot=True, fmt='g', ax=axis, cmap='Blues')
ax.set_xlabel("Predicted Label", fontsize=14)
ax.set_ylabel("Original Label", fontsize=14)
ax.set_title("Confusion Matrix", fontsize=14)

"""##Conclusion

We can see **'1'** is having the best predictive result(**most confident**) compared to other digits as it has the **least complication** and our model can predict almost every test data correctly whereas **'5'** is being the **least confident** among others as the digit itself has **complicated features**, so it is a little hard to perdict compared to other digits.
"""